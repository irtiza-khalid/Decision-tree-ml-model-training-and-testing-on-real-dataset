{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c178a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with entropy: 76.58959537572254%\n",
      "Accuracy with gini: 76.58959537572254%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"car.csv\")\n",
    "col_names=[\"buying\",\"maint\",\"doors\",\"persons\",\"lugg_boot\",\"saftey\",\"class\"]\n",
    "df.columns=col_names\n",
    "\n",
    "\n",
    "#converting in integers\n",
    "le=preprocessing.LabelEncoder()\n",
    "for col in df.columns:\n",
    "    df[col]=le.fit_transform(df[col])\n",
    "X=df.drop(\"class\",axis=1)\n",
    "y=df[\"class\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20)\n",
    "my_model_entropy=DecisionTreeClassifier(criterion=\"entropy\",max_depth=3,random_state=0)\n",
    "my_model_entropy.fit(X_train,y_train)\n",
    "y_pred_entropy=my_model_entropy.predict(X_test)\n",
    "accuracy_entropy=accuracy_score(y_pred_entropy,y_test)\n",
    "print(\"Accuracy with entropy: {}%\".format(accuracy_entropy*100))\n",
    "\n",
    "\n",
    "my_model_gini=DecisionTreeClassifier(criterion=\"gini\",max_depth=3,random_state=0)\n",
    "my_model_gini.fit(X_train,y_train)\n",
    "y_pred_gini=my_model_gini.predict(X_test)\n",
    "accuracy_gini=accuracy_score(y_pred_gini,y_test)\n",
    "print(\"Accuracy with gini: {}%\".format(accuracy_gini*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ee6769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in Class: 3\n",
      "Number of unique values in left_weight: 5\n",
      "Number of unique values in left_distance: 5\n",
      "Number of unique values in right_weight: 5\n",
      "Number of unique values in right_distance: 5\n",
      "\n",
      "Null values in dataset: 0\n",
      "\n",
      "Mean:\n",
      " left_weight       3.0\n",
      "left_distance     3.0\n",
      "right_weight      3.0\n",
      "right_distance    3.0\n",
      "dtype: float64\n",
      "\n",
      "Median:\n",
      " left_weight       3.0\n",
      "left_distance     3.0\n",
      "right_weight      3.0\n",
      "right_distance    3.0\n",
      "dtype: float64\n",
      "\n",
      "Standard Deviation:\n",
      " left_weight       1.415346\n",
      "left_distance     1.415346\n",
      "right_weight      1.415346\n",
      "right_distance    1.415346\n",
      "dtype: float64\n",
      "\n",
      "Accuracy score of Decision Tree with criterion 'gini': 0.7712765957446809\n",
      "Accuracy score of Decision Tree with criterion 'entropy': 0.776595744680851\n",
      "The model with criterion 'entropy' has better accuracy.\n",
      "\n",
      "Metrics for Decision Tree with criterion 'gini':\n",
      "Precision: 0.772887247131368\n",
      "Recall: 0.7712765957446809\n",
      "F-1 Score: 0.7669366434304643\n",
      "\n",
      "Metrics for Decision Tree with criterion 'entropy':\n",
      "Precision: 0.7778685177623521\n",
      "Recall: 0.776595744680851\n",
      "F-1 Score: 0.7734370394843607\n",
      "\n",
      "Confusion matrix for Decision Tree with criterion 'gini':\n",
      " [[ 0 14  4]\n",
      " [ 4 74  2]\n",
      " [12  7 71]]\n",
      "\n",
      "Confusion matrix for Decision Tree with criterion 'entropy':\n",
      " [[ 0 14  4]\n",
      " [ 7 72  1]\n",
      " [ 9  7 74]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWishHy\\AppData\\Local\\Temp\\ipykernel_13360\\2326032982.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(\"\\nMean:\\n\", df.mean())\n",
      "C:\\Users\\SWishHy\\AppData\\Local\\Temp\\ipykernel_13360\\2326032982.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(\"\\nMedian:\\n\", df.median())\n",
      "C:\\Users\\SWishHy\\AppData\\Local\\Temp\\ipykernel_13360\\2326032982.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(\"\\nStandard Deviation:\\n\", df.std())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv(\"balance-scale.csv\", header=None)\n",
    "\n",
    "# Add column names\n",
    "df.columns = ['Class', 'left_weight', 'left_distance', 'right_weight', 'right_distance']\n",
    "\n",
    "# Perform exploratory data analysis\n",
    "for col in df.columns:\n",
    "    print(f\"Number of unique values in {col}: {df[col].nunique()}\")\n",
    "print(f\"\\nNull values in dataset: {df.isnull().sum().sum()}\")\n",
    "print(\"\\nMean:\\n\", df.mean())\n",
    "print(\"\\nMedian:\\n\", df.median())\n",
    "print(\"\\nStandard Deviation:\\n\", df.std())\n",
    "\n",
    "# Choose features and label\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Convert categorical data to numerical data using label encoding\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build Decision Tree Classification models\n",
    "dt_gini = DecisionTreeClassifier(criterion='gini')\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# Train both models on the training set\n",
    "dt_gini.fit(X_train, y_train)\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_gini = dt_gini.predict(X_test)\n",
    "y_pred_entropy = dt_entropy.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of both models and compare them\n",
    "acc_gini = accuracy_score(y_test, y_pred_gini)\n",
    "acc_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "print(f\"\\nAccuracy score of Decision Tree with criterion 'gini': {acc_gini}\")\n",
    "print(f\"Accuracy score of Decision Tree with criterion 'entropy': {acc_entropy}\")\n",
    "print(\"The model with criterion 'entropy' has better accuracy.\")\n",
    "\n",
    "# Calculate precision, recall, and f-1 score using SKLEARN library\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "print(f\"\\nMetrics for Decision Tree with criterion 'gini':\\nPrecision: {precision_gini}\\nRecall: {recall_gini}\\nF-1 Score: {f1_gini}\")\n",
    "print(f\"\\nMetrics for Decision Tree with criterion 'entropy':\\nPrecision: {precision_entropy}\\nRecall: {recall_entropy}\\nF-1 Score: {f1_entropy}\")\n",
    "\n",
    "# Draw the confusion matrix\n",
    "cm_gini = confusion_matrix(y_test, y_pred_gini)\n",
    "cm_entropy = confusion_matrix(y_test, y_pred_entropy)\n",
    "\n",
    "print(\"\\nConfusion matrix for Decision Tree with criterion 'gini':\\n\", cm_gini)\n",
    "print(\"\\nConfusion matrix for Decision Tree with criterion 'entropy':\\n\", cm_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50c212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
